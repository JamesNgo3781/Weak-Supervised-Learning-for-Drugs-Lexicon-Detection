# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dMgHJhT3cnmDrpGeDV5IdLqTb_sNNiPa
"""

!pip install -q tfds-nightly
!pip install nltk
import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow.keras import regularizers, initializers, optimizers, callbacks
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.utils.np_utils import to_categorical
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
import pandas as pd
import numpy as np
import re
from tqdm import tqdm_notebook
import nltk
nltk.download('stopwords')
from sklearn.model_selection import train_test_split

df = pd.read_csv('train.csv', header = None)
train = df
labels = [2,3]
y = train[labels].values

def clean_text(text, remove_stopwords = True):
    output = ""
    text = str(text).replace("\n", "")
    text = re.sub(r'[^\w\s]','',text).lower()
    text = re.sub('[0-9]', '', text)
    if remove_stopwords:
        text = text.split(" ")
        for word in text:
            if word not in stopwords.words("english"):
                output = output + " " + word
    else:
        output = text
    return str(output.strip())[1:-3].replace("  ", " ")
text_train = train[1]
text_train = list(text_train)
texts = [] 
for line in tqdm_notebook(text_train): 
    texts.append(clean_text(line))

tokenizer = Tokenizer(num_words=100000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index
data = pad_sequences(sequences, padding = 'post', maxlen = 250)

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = y[indices]

num_validation_samples = int(0.3*data.shape[0])
x_train = data[: -num_validation_samples].astype('float64')
y_train = labels[: -num_validation_samples].astype('float64')
x_val = data[-num_validation_samples: ].astype('float64')
y_val = labels[-num_validation_samples: ].astype('float64')

embeddings_index = {}
f = open("glove.6B."+str(50)+"d.txt")
for line in f:
    values = line.split()
    word = values[0]
    embeddings_index[word] = np.asarray(values[1:], dtype='float32')
f.close()
embedding_matrix = np.random.random((len(word_index) + 1, 50))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector



sequence_input = Input(shape=(250,), dtype='int32')
embedding_layer = Embedding(len(word_index) + 1,
                           50,
                           weights = [embedding_matrix],
                           input_length = 250,
                           trainable=False,
                           name = 'embeddings')
embedded_sequences = embedding_layer(sequence_input)
embedding_matrix.shape

x = LSTM(50, return_sequences=True,name='lstm_layer')(embedded_sequences)
x = GlobalMaxPool1D()(x)
x = Dropout(0.2)(x)
preds = Dense(2, activation="softmax")(x)

model = Model(sequence_input, preds)
model.compile(loss = 'binary_crossentropy',
             optimizer='adam',
             metrics = ['accuracy'])

history = model.fit(x_train, y_train, epochs = 20, batch_size= 8, validation_data=(x_val, y_val))

model.evaluate(x_train, y_train, verbose=0)